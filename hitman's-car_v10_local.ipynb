{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Welcome, To ~ \n",
        "#   The Hitman's car_______\"(0TYT=0> ______________version-10\n",
        "#___________________________________________________________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ~~ Setting Everything Up /...\n",
        "#________________________________________________________________________________________________\n",
        "#source_vid_path = \"/content/drive/MyDrive/Final-Task/Data/Obama-db/obamawalk.mp4\"\n",
        "#source_vid_path = \"data/workshop-vid.mp4\"\n",
        "#source_vid_path = \"data/obamawalk.mp4\"\n",
        "source_vid_path = 0\n",
        "#________________________________________________________________________________________________\n",
        "#vid_save_path = \"/content/drive/MyDrive/Final-Task/Data/Obama-db/obama-output.mp4\"\n",
        "vid_save_path = \"./outputs/r.mp4\"\n",
        "#________________________________________________________________________________________________\n",
        "#target_face_path = \"/content/drive/MyDrive/Final-Task/Data/Obama-db/obama2.jpg\"\n",
        "#target_face_path = \"obama2.jpg\"\n",
        "target_face_path = \"./me.jpg\"\n",
        "#________________________________________________________________________________________________\n",
        "#Sensitivity to recognized person's movements\n",
        "#sensitivity = 0.001    \n",
        "sensitivity = 0.0001\n",
        "#________________________________________________________________________________________________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_qN6L4U91nBg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "from deepface import DeepFace\n",
        "from deepface.commons import functions\n",
        "from deepface.detectors import FaceDetector\n",
        "from matplotlib import pyplot as plt\n",
        "import cmapy\n",
        "#from facenet_pytorch import InceptionResnetV1, MTCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hliL-92U1nBi"
      },
      "outputs": [],
      "source": [
        "# ~~ The deepface package ~~\n",
        "\n",
        "#Face detection ~\n",
        "backends = [\n",
        "  'opencv', \n",
        "  'ssd', \n",
        "  'dlib', \n",
        "  'mtcnn', \n",
        "  'retinaface', \n",
        "  'mediapipe'\n",
        "]\n",
        "\n",
        "#Face recognition ~\n",
        "models = [\n",
        "  \"VGG-Face\", \n",
        "  \"Facenet\", \n",
        "  \"Facenet512\", \n",
        "  \"OpenFace\", \n",
        "  \"DeepFace\", \n",
        "  \"DeepID\", \n",
        "  \"ArcFace\", \n",
        "  \"Dlib\", \n",
        "  \"SFace\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UxFHOlfa1nBi",
        "outputId": "7427e3fe-5115-4799-fe0f-e4e49620defc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras.engine.functional.Functional at 0x1343d63d810>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ~~ The Models used in this project ~~\n",
        "\n",
        "#Objct detection - to detect people---------------------------------------------#\n",
        "#seg_model = YOLO('yolov8n-seg.pt')\n",
        "yolo_model = YOLO('yolov8n.pt')\n",
        "\n",
        "#Face detection ~ --------------------------------------------------------------#\n",
        "face_detector = FaceDetector.build_model('ssd')\n",
        "'''face_detector = MTCNN(\n",
        "    image_size=299, \n",
        "    margin=40, \n",
        "    min_face_size=20,\n",
        "    keep_all=False,\n",
        "    )'''\n",
        "\n",
        "#Face recognition ~ -----------------------------------------------------------#\n",
        "DeepFace.build_model('SFace')\n",
        "DeepFace.build_model('Facenet512')\n",
        "#face_recognizer = InceptionResnetV1(pretrained='vggface2').eval()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Iko0RG0B1nBj",
        "outputId": "53ccbc5a-0fac-42f4-c955-811678836b18"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\HP/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading weights:  None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\HP/.cache\\torch\\hub\\rwightman_gen-efficientnet-pytorch_master\n",
            "Using cache found in C:\\Users\\HP/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
          ]
        }
      ],
      "source": [
        "# ~~ MiDaS - Depth Estimation ~~\n",
        "\n",
        "model_type = \"MiDaS_small\"  # MiDaS v2.1 - Small   (lowest accuracy, highest inference speed)\n",
        "#model_type = \"DPT_Hybrid\"   # MiDaS v3 - Hybrid    (medium accuracy, medium inference speed)\n",
        "#model_type = \"DPT_Large\"     # MiDaS v3 - Large     \n",
        "\n",
        "# Download model if not yet downloaded\n",
        "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
        "\n",
        "# Set torch options\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "midas.to(device)\n",
        "midas.eval()\n",
        "\n",
        "# Load transforms for each model\n",
        "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
        "\n",
        "# Select transforms\n",
        "if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
        "    transform = midas_transforms.dpt_transform\n",
        "else:\n",
        "    transform = midas_transforms.small_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yB4Y5Rdw1nBj",
        "outputId": "ebcdfe88-833a-4a20-a3ea-8142c8703447"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\ndef cosine(emb1, emb2):\\n    cos = np.dot(emb1, emb2.T) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\\n    return cos'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def yolov8(frame):\n",
        "    results = yolo_model.predict(\n",
        "        source = frame,\n",
        "        \n",
        "        conf = 0.5,\n",
        "        iou = 0.7,\n",
        "        show = False,\n",
        "        save = False,\n",
        "        retina_masks = True,\n",
        "    )\n",
        "    return results  \n",
        "\n",
        "def get_cropped_people(results, frame):\n",
        "    '''\n",
        "    results is from yolov8\n",
        "    Returns list of cropped images of persons, and corresponding list of their top left coordinates\n",
        "    '''\n",
        "    im_list = []\n",
        "    coordi_list = []\n",
        "    area_list = []\n",
        "    for result in results:\n",
        "        for box in result.boxes:\n",
        "            for a, b, c, d in box.xyxy:\n",
        "                x1 = int(a)\n",
        "                y1 = int(b)\n",
        "                x2 = int(c)\n",
        "                y2 = int(d)\n",
        "            croppped = frame[y1:y2, x1:x2]\n",
        "            area_list.append((x2-x1)*(y2-y1))\n",
        "            im_list.append(croppped)\n",
        "            \n",
        "            '''x = (x1 + x2)/2\n",
        "            y = (y1 + y2)/2'''\n",
        "            coordi_list.append((x1, y1))\n",
        "    return im_list, coordi_list, area_list\n",
        "\n",
        "def get_cropped_faces(people_list, coordi_list, area_list):\n",
        "    '''\n",
        "    people_list is from get_cropped_people\n",
        "    Returns list of cropped images of faces \n",
        "    & corrsponding list of bounding boxes in main frame\n",
        "    '''\n",
        "    face_list = []\n",
        "    bounding_box_list = []\n",
        "    new_area_list = []\n",
        "\n",
        "    for i, person in enumerate(people_list):\n",
        "\n",
        "        x_person, y_person = coordi_list[i]\n",
        "        area = area_list[i]\n",
        " \n",
        "        if person.shape[0] == 0 or person.shape[1] == 0 or person is None:\n",
        "            continue\n",
        "        try:       \n",
        "            face_objs = FaceDetector.detect_faces(\n",
        "                face_detector = face_detector,\n",
        "                detector_backend = 'ssd',\n",
        "                img = person,\n",
        "                align = True,\n",
        "            )\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        \n",
        "        if face_objs is None:\n",
        "            print('No face detected')\n",
        "            continue\n",
        "\n",
        "        test_count = 0\n",
        "        for im_obj in face_objs:\n",
        "            face, box, conf = im_obj\n",
        "            face_list.append(face)\n",
        "\n",
        "            '''print('Face detected')\n",
        "            \n",
        "            print(box)\n",
        "            print(test_count)\n",
        "            test_count += 1'''\n",
        "\n",
        "            \n",
        "            '''cv.imshow('face', face)\n",
        "            cv.waitKey(0)\n",
        "            cv.destroyAllWindows()'''\n",
        "            \n",
        "            x1 = int(box[0])\n",
        "            y1 = int(box[1])\n",
        "            x2 = int(box[0] + box[2])\n",
        "            y2 = int(box[1] + box[3])\n",
        "\n",
        "            bounding_box_p1 = (\n",
        "                x_person + x1,\n",
        "                y_person + y1,\n",
        "            )\n",
        "            bounding_box_p2 = (\n",
        "                x_person + x2,\n",
        "                y_person + y2,\n",
        "            )\n",
        "            #print(bounding_box_p1, bounding_box_p2)\n",
        "            bounding_box_list.append((bounding_box_p1, bounding_box_p2))\n",
        "\n",
        "            #Area of the person-box from yolo, corresponding to this (i'th) face\n",
        "            new_area_list.append(area)\n",
        "            \n",
        "    return face_list, bounding_box_list, new_area_list\n",
        "\n",
        "\n",
        "def get_depth(img):\n",
        "    input_batch = transform(img).to(device)\n",
        "    with torch.no_grad():\n",
        "        prediction = midas(input_batch)\n",
        "\n",
        "        prediction = torch.nn.functional.interpolate(\n",
        "            prediction.unsqueeze(1),\n",
        "            size=img.shape[:2],\n",
        "            mode=\"bicubic\",\n",
        "            align_corners=False,\n",
        "        ).squeeze()\n",
        "\n",
        "    output = prediction.cpu().numpy()\n",
        "\n",
        "    output /= output.max()\n",
        "    output *= 255\n",
        "    output = output.astype(np.uint8)\n",
        "    output = cv.applyColorMap(output, cmapy.cmap('magma'))\n",
        "\n",
        "    return output\n",
        "\n",
        "def face_recognition(target_face, face_pic, model_name=\"Facenet\", detector_backend=\"skip\"):\n",
        "    result = DeepFace.verify(\n",
        "        target_face, \n",
        "        face_pic, \n",
        "        model_name, \n",
        "        detector_backend\n",
        "        )\n",
        "    return result\n",
        "\n",
        "def findCosineDistance(source_representation, test_representation):\n",
        "    a = np.matmul(np.transpose(source_representation), test_representation)\n",
        "    b = np.sum(np.multiply(source_representation, source_representation))\n",
        "    c = np.sum(np.multiply(test_representation, test_representation))\n",
        "    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))\n",
        "\n",
        "'''\n",
        "def cosine(emb1, emb2):\n",
        "    cos = np.dot(emb1, emb2.T) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
        "    return cos'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7yEajMG41nBk",
        "outputId": "490abc41-7e4b-46d7-dc7e-0c78031924b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cv.imshow(\\'target\\', target_face)\\ncv.waitKey(0)\\ncv.destroyAllWindows()\\n\\ntarget_rep = target_embd[0][\"embedding\"]'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "#Get target face~ \n",
        "target_face = cv.imread(target_face_path)\n",
        "\n",
        "'''cv.imshow('target', target_face)\n",
        "cv.waitKey(0)\n",
        "cv.destroyAllWindows()\n",
        "\n",
        "target_rep = target_embd[0][\"embedding\"]'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VIrtnMca1nBk",
        "outputId": "4ebf61af-1d9c-4c23-bf80-d0a15fae2467"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 (no detections), 230.4ms\n",
            "Speed: 3.3ms preprocess, 230.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  0\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0: 480x640 1 person, 161.4ms\n",
            "Speed: 9.2ms preprocess, 161.4ms inference, 7.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 480x640 1 person, 139.5ms\n",
            "Speed: 0.0ms preprocess, 139.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  0\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 136.8ms\n",
            "Speed: 0.0ms preprocess, 136.8ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  0\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 136.4ms\n",
            "Speed: 0.0ms preprocess, 136.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  0\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 126.0ms\n",
            "Speed: 0.0ms preprocess, 126.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  0\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 134.3ms\n",
            "Speed: 0.0ms preprocess, 134.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  0\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 (no detections), 126.2ms\n",
            "Speed: 0.0ms preprocess, 126.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  0\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 (no detections), 143.3ms\n",
            "Speed: 0.0ms preprocess, 143.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  0\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 (no detections), 134.4ms\n",
            "Speed: 0.0ms preprocess, 134.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  0\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 (no detections), 124.5ms\n",
            "Speed: 0.0ms preprocess, 124.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  1.929\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 laptop, 179.6ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  1.929\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Speed: 0.0ms preprocess, 179.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 480x640 (no detections), 128.7ms\n",
            "Speed: 0.0ms preprocess, 128.7ms inference, 11.3ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  1.929\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 (no detections), 128.9ms\n",
            "Speed: 0.0ms preprocess, 128.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  1.929\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 laptop, 143.4ms\n",
            "Speed: 0.0ms preprocess, 143.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  1.929\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 (no detections), 127.0ms\n",
            "Speed: 0.0ms preprocess, 127.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  1.929\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 (no detections), 143.8ms\n",
            "Speed: 0.0ms preprocess, 143.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  1.929\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 (no detections), 156.9ms\n",
            "Speed: 0.0ms preprocess, 156.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  1.929\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 163.7ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  1.929\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Speed: 0.0ms preprocess, 163.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  1.929\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0: 480x640 1 person, 219.1ms\n",
            "Speed: 0.9ms preprocess, 219.1ms inference, 7.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  2.504\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0: 480x640 1 person, 204.6ms\n",
            "Speed: 0.0ms preprocess, 204.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 480x640 1 person, 147.2ms\n",
            "Speed: 0.0ms preprocess, 147.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  2.504\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  2.504\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0: 480x640 1 person, 159.3ms\n",
            "Speed: 1.6ms preprocess, 159.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 480x640 1 person, 149.2ms\n",
            "Speed: 0.0ms preprocess, 149.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  2.504\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 143.1ms\n",
            "Speed: 0.0ms preprocess, 143.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  2.504\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 155.0ms\n",
            "Speed: 0.0ms preprocess, 155.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  2.504\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  2.504\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0: 480x640 1 person, 165.2ms\n",
            "Speed: 4.0ms preprocess, 165.2ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 480x640 1 person, 124.2ms\n",
            "Speed: 0.0ms preprocess, 124.2ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  2.504\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 148.3ms\n",
            "Speed: 0.0ms preprocess, 148.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  2.504\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 134.8ms\n",
            "Speed: 0.0ms preprocess, 134.8ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  2.504\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 135.2ms\n",
            "Speed: 0.0ms preprocess, 135.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  2.252\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 143.0ms\n",
            "Speed: 0.0ms preprocess, 143.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  2.252\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 142.6ms\n",
            "Speed: 0.0ms preprocess, 142.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  2.252\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  2.252\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0: 480x640 1 person, 154.9ms\n",
            "Speed: 3.5ms preprocess, 154.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 480x640 1 person, 143.2ms\n",
            "Speed: 0.0ms preprocess, 143.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  2.252\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 140.5ms\n",
            "Speed: 0.0ms preprocess, 140.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  2.252\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 144.4ms\n",
            "Speed: 0.0ms preprocess, 144.4ms inference, 4.4ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  2.252\n",
            "0.0 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 1 person, 137.2ms\n",
            "Speed: 0.0ms preprocess, 137.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FPS =  2.252\n",
            "0.0 %\n",
            "FPS =  2.252\n"
          ]
        }
      ],
      "source": [
        "#Run the video, and go frame by frame ~\n",
        "#cap = cv.VideoCapture('workshop-vid.mp4')\n",
        "#cap = cv.VideoCapture('obama-biden.mp4')\n",
        "#cap = cv.VideoCapture('obama-bgface.mp4')\n",
        "#cap = cv.VideoCapture('obama-laterals-and-more.mp4')\n",
        "# cap = cv.VideoCapture('obamawalk.mp4')\n",
        "cap = cv.VideoCapture(0)\n",
        "#source_vid_path = \"data/biker.mp4\"\n",
        "#cap = cv.VideoCapture(source_vid_path)\n",
        "\n",
        "#FPS Calculation-----------------------------------------------------------------------------#\n",
        "frame_count = 0\n",
        "fps = 0\n",
        "\n",
        "#Video writer-------------------------------------------------------------------------------#\n",
        "frame_width = int(cap.get(3))\n",
        "frame_height = int(cap.get(4))*2\n",
        "   \n",
        "size = (frame_width, frame_height)\n",
        "\n",
        "output = cv.VideoWriter(vid_save_path, \n",
        "                         cv.VideoWriter_fourcc(*'mp4v'),\n",
        "                         5, size)\n",
        "#--------------------------------------------------------------------------------------------#\n",
        "previous_area = -1\n",
        "\n",
        "#Main loop---------------------------------------------------------------------------------#\n",
        "tic = time.time()\n",
        "while True:\n",
        "    \n",
        "    ret, frame = cap.read()\n",
        "    frame_count = frame_count + 1\n",
        "\n",
        "    nextFrameNo = cap.get(cv.CAP_PROP_POS_FRAMES)\n",
        "    totalFrames = cap.get(cv.CAP_PROP_FRAME_COUNT)\n",
        "    complete = nextFrameNo/+1\n",
        "    print(complete*100, '%')\n",
        "\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    if np.any(frame) == False:\n",
        "        continue\n",
        "    \n",
        "    # Person detector---------------------------------------------------------------------------------#\n",
        "    results = yolov8(frame)\n",
        "    cropped_people, coordi_list, area_l = get_cropped_people(results, frame)\n",
        "    output_frame = results[0].plot() \n",
        "    \n",
        "    # Face detector------------------------------------------------------------------------------------#\n",
        "    if len(cropped_people) != 0:\n",
        "        face_list, bounding_boxes, person_areas = get_cropped_faces(cropped_people, coordi_list, area_l)\n",
        "        \n",
        "        #Boxes on faces\n",
        "        for i, face in enumerate(face_list):\n",
        "            p1, p2 = bounding_boxes[i]\n",
        "            #print('Drawing box', p1, p2)\n",
        "            cv.rectangle(output_frame, p1, p2, (0, 255, 255), 2)\n",
        "\n",
        "        #Face recognition--------------------------------------------------------------------------------#\n",
        "        face_model = \"SFace\"\n",
        "        \n",
        "        for i, face_pic in enumerate(face_list):\n",
        "            face = face_pic.copy()\n",
        "            if np.any(face) == False:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                result = face_recognition(\n",
        "                    target_face,\n",
        "                    face,\n",
        "                    model_name = face_model,\n",
        "                )\n",
        "                print(\"using - \", face_model)\n",
        "            except:\n",
        "                continue\n",
        "            \n",
        "\n",
        "            # We found our guy ??????????????\n",
        "            if (result['verified'] == True ):\n",
        "                p1, p2 = bounding_boxes[i]\n",
        "                cv.rectangle(output_frame, p1, p2, (255, 0, 20), 2)\n",
        "                #cv.putText(output_frame, 'found u!', p1, cv.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 20), 2, cv.LINE_AA)\n",
        "\n",
        "                if previous_area != -1:\n",
        "\n",
        "                    diff = person_areas[i] - previous_area\n",
        "\n",
        "                    threshold = frame.shape[0] * frame.shape[1] * sensitivity\n",
        "\n",
        "                    if diff > threshold:\n",
        "                        cv.putText(output_frame, 'Oh, You are approaching me?', p1, cv.FONT_HERSHEY_SIMPLEX, 1, (54, 255, 2), 2, cv.LINE_AA)\n",
        "                    elif diff < -threshold:\n",
        "                        cv.putText(output_frame, 'You going away?', p1, cv.FONT_HERSHEY_SIMPLEX, 1, (54, 255, 2), 2, cv.LINE_AA)\n",
        "                    else:\n",
        "                        cv.putText(output_frame, 'You not gonna move now?', p1, cv.FONT_HERSHEY_SIMPLEX, 1, (54, 255, 2), 2, cv.LINE_AA)\n",
        "                        \n",
        "                previous_area = person_areas[i]\n",
        "                break\n",
        "        \n",
        "\n",
        "#------------------------------------------------------------------------------------------------#\n",
        "    #Depth map\n",
        "    depth = get_depth(frame)\n",
        "    #cv.imshow('depth', depth)\n",
        "    output_frame = np.concatenate((output_frame, depth), axis=0)\n",
        "#------------------------------------------------------------------------------------------------#\n",
        "    #Output ~\n",
        "\n",
        "    if frame_count%10 == 0:\n",
        "        toc = time.time()\n",
        "        fps = round((frame_count / (toc - tic)), 3)\n",
        "        frame_count = 0\n",
        "        tic = toc\n",
        "        \n",
        "    print('FPS = ', fps)\n",
        "    \n",
        "    cv.putText(\n",
        "        output_frame,\n",
        "        f'FPS: {fps}',\n",
        "        (10, 50),\n",
        "        cv.FONT_HERSHEY_SIMPLEX,\n",
        "        1,\n",
        "        (255, 0, 125),\n",
        "        2,\n",
        "        cv.LINE_AA,     \n",
        "    )\n",
        "\n",
        "    output.write(output_frame)\n",
        "    cv.imshow(\"Output\", output_frame)\n",
        "\n",
        "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "    \n",
        "\n",
        "cap.release()\n",
        "output.release()\n",
        "cv.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
